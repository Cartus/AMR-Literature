# AMR-Literature
This repo contains relevant resources related to AMR (Abstract Meaning Representation).


## AMR Parsing

* A differentiable relaxation of graph segmentation and alignment for amr parsing. 
[[Paper]](https://arxiv.org/pdf/2010.12676.pdf)
**EMNLP2021**
* Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing. **EMNLP2021**
* Stacked AMR Parsing with Silver Data. **Findings ACL2021**
* Ensembling Graph Predictions for AMR Parsing. **NeurIPS2021**
* AMR parsing with action-pointer transformer. **NAACL2021**
* One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline. AAAI2021
* Improving AMR Parsing with Sequence-to-Sequence Pre-training. **EMNLP2020**
* Fast semantic parsing with well-typedness guarantees. **EMNLP2020**
* Pushing the Limits of AMR Parsing with Self-Learning. **Findings EMNLP2020**
* Transition-based Parsing with Stack-Transformers. **Findings EMNLP2020**
* AMR Parsing via Graph-Sequence Iterative Inference. 
[[Paper]](https://arxiv.org/pdf/2010.12676.pdf)
[[Code]](https://github.com/jcyk/AMR-gs)
[[Bib]](https://aclanthology.org/2020.acl-main.119.bib)
**ACL2020**
* Core Semantic First: A Top-down Approach for AMR Parsing. **EMNLP2019**
* Broad-coverage semantic parsing as transduction. **EMNLP2019**
* AMR parsing as sequence-to-graph transduction. **ACL2019**
* Rewarding Smatch: Transition-based AMR parsing with reinforcement learning. **ACL2019**
* Better transition-based AMR parsing with a refined search space. **EMNLP2018**
* AMR parsing as graph prediction with latent alignment. **ACL2018**
* AMR parsing using stack-LSTMs. **EMNLP2017**
* Getting the most out of amr parsing. **EMNLP2017**
* Robust incremental neural semantic graph parsing. **ACL2017**
* An incremental parser for abstract meaning representation. **EACL2016**
* A transition-based algorithm for AMR parsing. **NAACL2015**


### Multilingual/Cross-Lingual AMR Parsing

* Multilingual AMR Parsing with Noisy Knowledge Distillation. **Findings EMNLP2021**
* Making Better Use of Bilingual Information for Cross-Lingual AMR Parsing. **Findings ACL2021**
* Translate, then Parse! A strong baseline for Cross-Lingual AMR Parsing. **IWPT2021**

## AMR-to-Text Generation


* Structural Adapters in Pretrained Language Models for AMR-to-Text Generation. **EMNLP2021**
* Smelting Gold and Silver for Improved Multilingual AMR-to-Text Generation. **EMNLP2021**
* Avoiding Overlap in Data Augmentation for AMR-to-Text Generation. **ACL2021**
* Stage-wise Fine-tuning for Graph-to-Text Generation. **ACL2021**
* Generating Landmark Navigation Instructions from Maps as a Graph-to-Text Problem. **ACL2021**
* XLPT-AMR: Cross-Lingual Pre-Training via Multi-Task Learning for Zero-Shot AMR Parsing and Text Generation.  **ACL2021**
* DART: Open-Domain Structured Data Record to Text Generation. **NAACL2021**
* Multilingual AMR-to-Text Generation. **EMNLP2020**
* Online Back-Parsing for AMR-to-Text Generation.  **EMNLP2020**
* Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks. **ACL2020**
* GPT-too: A language-model-first approach for AMR-to-text generation.  **ACL2020**
* AMR-To-Text Generation with Graph Transformer. **TACL2020**
* Graph Transformer for Graph-to-Sequence Learning.  **AAAI2020**
* Heterogeneous Graph Transformer for Graph-to-Sequence Learning.  **ACL2020**
* structural information preserving for graph-to-text generation.  **ACL2020**
* Generalized Shortest-Paths Encoders for AMR-to-Text Generation.  **COLING2020**
* Towards a Decomposable Metric for Explainable Evaluation of Text Generation from AMR.  **EACL2020**
* Modeling Graph Structure in Transformer for Better AMR-to-Text Generation.   **EMNLP2019**
* Enhancing AMR-to-Text Generation with Dual Graph Representations.  **EMNLP2019**

## Applications using AMR

* A Semantics-aware Transformer Model of Relation Linking for Knowledge Base Question Answering. **ACL2021**
* Leveraging Abstract Meaning Representation for knowledge base question answering. **Findings ACL2021**


